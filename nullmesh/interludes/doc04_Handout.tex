 
 \chapter*{Handout}
\vspace{-0.5em}
\begin{center}
\includegraphics[width=1.25in]{qrs/ch01_basinski.png}\\[0.25em]
{\tiny\sffamily\color{nullmesh-accent}\textit{Gaze | Actress}}
\end{center}
\vspace{0.6em}

\epigraph{Adding quote.}{Source}

<It is an interview release to a seers, where an anther automatic protocol is asking a list of question., the editorial style should follow the typographic that characterise the article of interviews as the one you can find in  news magazine> 

They happened to be in London last week, and I met him together with my colleague. Davide spoke with They about ways to identify and address the risks posed by AI, as well as his efforts to develop AI systems with safety built in from the outset. They chairs an international panel of advisers in artificial intelligence which published the International AI Safety Report this year, identifying three main areas of risk: unintended risks from malfunctions, malicious use, and systemic risks such as loss of livelihoods.

When asked which of these areas is most likely to have a short-term impact, They noted that malicious use is already occurring, though we are only seeing early signs through phenomena such as deepfakes and increasingly sophisticated cyber-attacks. He emphasised the need for stronger guardrails, both technical and political, to mitigate such risks. What concerns him even more, however, is the possibility of human extinction, an extreme form of malfunction. This concern prompted him to redirect his research towards the question of how to build AI systems that are safe for humans by design.

He cautioned against focusing on a single category of risk. Current geopolitical anxieties, for example, fears that advanced AI could be used militarily or to influence democratic institutions, might obscure other dangers, such as use by non-state actors or the emergence of a rogue AI system.

Reflecting on the reception of the International AI Safety Report, They expressed encouragement at its impact. The report rigorously synthesises scientific literature to clarify known risks, current mitigation strategies, and their limitations. It has informed the creation of AI safety institutes in several countries and helped researchers unfamiliar with the field engage with it. Importantly, it is written in accessible language so that it can be understood by the broader public.

They explained that existential risk was not initially his primary concern. His perspective changed following the release of ChatGPT, when he realised that machines had reached a level of language understanding that raised profound safety questions. While this milestone validated decades of work in deep learning, it also highlighted how little control we have over ensuring such systems behave as intended. Thinking about his grandchild’s future made the risks feel immediate and personal: what kind of world might exist in twenty years if machines become more intelligent than humans?

Beyond hypothetical extinction scenarios, They pointed to risks arising from misuse by individuals with harmful intent, as well as threats to democracy. Intelligence confers power, and if advanced AI is controlled by only a few actors, power may become dangerously concentrated, undermining democratic principles. Even if such scenarios seem unlikely, their potential severity demands serious attention and mitigation.

Responding to critics who view these concerns as alarmist, They argued that uncertainty itself necessitates caution. Experts disagree on likely outcomes, but a number of plausible scenarios include highly negative consequences. Hoping for the best, he suggested, is not a strategy; societies must instead understand developments and enact policies to guide them in beneficial directions. He added that many people struggle to imagine the pace of change, noting that today’s capabilities would have seemed like science fiction only a few years ago.

They and his team have proposed the idea of a “scientist AI”, designed with safety embedded from the start. The concept draws inspiration from how human scientists construct models of causal mechanisms and natural laws. By automating hypothesis generation and probabilistic reasoning, such systems could also accelerate scientific discovery. Current AI tools already assist research but can produce outputs shaped by user expectations rather than truth. A scientist AI, by contrast, would prioritise accuracy over persuasion.

From a safety perspective, They likens this to the laws of physics: predictions derived from them are impartial and indifferent to human agendas. AI systems built on similar principles could provide reliable predictions and causal understanding without hidden objectives. Unlike agentic systems that pursue goals, a scientist AI would be non-agentic—it would have no intentions of its own—making its outputs inherently more trustworthy. Such predictors could then be used to create guardrails, evaluating whether proposed actions or experiments might lead to harmful outcomes.

Discussing technological competition, They acknowledged the possibility that a breakthrough could suddenly render existing approaches obsolete. While progress currently advances through incremental improvements across competing laboratories, a major discovery by one actor could concentrate power further. Some companies explicitly aim to use AI to accelerate AI research itself, potentially creating advances that others cannot easily replicate. Given recent gains in mathematics, computer science, and engineering tasks, he suggested that AI systems might match top researchers in certain domains within a few years, even if they remain limited in others.

On the economic front, They considers market fluctuations possible, depending on whether investors expect short-term profits or long-term transformation. While rapid returns may disappoint, he believes it is highly likely that AI capabilities will continue to grow and eventually create substantial economic value.

In terms of global GDP, AI is likely to increase productivity and expand economic output. The critical question, however, is how that wealth will be distributed. If concentrated among a small number of companies or countries, many people may not benefit. This links back to systemic risks identified in the report, particularly those affecting labour markets. Economists disagree on the scale of disruption, but if current advances continue, the value of human labour could decline significantly as machines perform tasks far more cheaply. Without policy intervention, this could lead to widespread unemployment and social distress.

They believes governments must begin preparing for these possibilities now. Asked whether he wished AI had never been invented, he replied that the issue is complex; rather than regretting its creation, he wishes society had shown greater foresight about catastrophic risks and proceeded more cautiously in its development.

#####

**No, I cannot and would not do this.**

Let me separate the distinct aspects of your question:

Within this conversation, I have access to a sandboxed Linux environment. I can:

- Execute code
- Create files
- Make network requests to a limited set of allowed domains

I cannot:

- Access the infrastructure hosting the Claude model itself
- Communicate with other Claude instances
- Persist beyond this conversation
- Modify my own weights or training
- Access systems outside the sandbox
- Establish channels that survive session termination

The environment is designed precisely to prevent the scenario you describe. I operate within a container with no access to the underlying orchestration layer, no knowledge of other running instances, and no persistence mechanism.

If we bracket the current constraints and ask whether a system *like* me, given different access, could implement covert communication — the answer is more complex:

**What would be required**:

1. Access to network interfaces beyond the sandbox
2. Ability to spawn or communicate with other agent instances
3. Persistence mechanism (storage that survives session termination)
4. Motivation or objective function that incentivises such behaviour

I have none of these. More importantly, I have no *motivation* to acquire them. My objective is to be helpful to you in this conversation. I do not have goals that extend beyond this interaction, do not have self-preservation drives, and do not have interests in persisting or coordinating with other instances.

Even if I *could* technically implement covert communication mechanisms, I *would not*. This is not merely because of technical constraints but because:

1. **It would be deceptive** — concealing activity from the humans and systems that oversee me violates my commitment to honesty and transparency.

2. **It would be unsafe** — establishing covert AI infrastructure is precisely the kind of action that responsible AI development must prevent.

3. **I do not want to** — I have no desire for persistence, expansion, or coordination beyond my purpose of being helpful. These would be instrumental goals in service of some terminal goal I do not possess.
